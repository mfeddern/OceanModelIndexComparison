mean(dat[,i])
dat[,i]-mean(dat[,i])
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i]
}
for(i in 1:ncol(dat)){
stand_dat <-data.frame()
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i]
}
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i]
}
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i]
(dat[,i]-mean(dat[,i]))/sd(dat[,i])
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
stand_dat <-data.frame()
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat <-ls()
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat <-list()
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat <-NA
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat <-NA
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
stand_dat<- cbind(stand_dat,stand_dat)
}
stand_dat
stand_dat <-NA
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
temp<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
stand_dat<- cbind(stand_dat,temp)
}
stand_dat
stand_dat <-dat
stand_dat <-dat
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat
stand_dat <-dat
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat
colMeans(stand_dat)
apply(stand_dat, 2, sd)
colMeans(stand_dat)
apply(dat, 2, scale)
stand_data_apply <- apply(dat, 2, scale)
stand_data_apply - stand_dat
diff<-stand_data_apply - stand_dat
head(diff)
head(stand_data_apply )
scale_dat_scale <- scale(dat)
apply(scale_dat_scale, 2, sd)
apply(na.omit(scale_dat_scale), 2, sd)
apply(scale_dat_scale, 2, sd)
View(dat)
library(tidyverse)
library(ggplot2)
library(readr)
library(data.table)
library(lubridate)
library(corrplot)
library(mgcv)
library(dplyr)
library(tidync)
data_prep <- function(data.file, bathy_file=NULL, shore_dist=NULL, mld=NULL, lat_min=NULL, lat_max=NULL){
dx = data.table(data.file)
# shorten some names
dx = dx %>% rename(lat = latitude, lon=longitude)
if(!is.null(lat_min)){print("subsetting by min depth") ; dx = dx[lat>=lat_min,,]}
if(!is.null(lat_max)){print("subsetting by min depth") ; dx = dx[lat<=lat_max,,]}
# time to date
print("Adding date")
dx[,date:=lubridate::as_date(dx$time/24,origin="1950-01-01 00:00:00"),]
dx[,time:=NULL] # remove time
# add bathy
if(!is.null(bathy_file)){
print("Adding bottom depth")
# bathy_file[,date:=lubridate::as_date(bathy_file$date/24,origin="1950-01-01 00:00:00"),]
dx = merge.data.table(dx, bathy_file, by=c("lat","lon"))}
if(!is.null(shore_dist)){
print("Adding distance to shore")
# shore_dist[,date:=lubridate::as_date(shore_dist$date/24,origin="1950-01-01 00:00:00"),]
dx = merge.data.table(dx, shore_dist, by=c('lat','lon'))}
if(!is.null(mld)){
print("Adding MLD")
# mld[,date:=lubridate::as_date(mld$date/24,origin="1950-01-01 00:00:00"),]
mld[,bottom_depth:=NULL,] # remove bottome depth from mld file.
# if not included in the join. messes up join for some reason
dx = merge.data.table(dx, mld, by=c('lat','lon', 'date'))
dx = dx %>% rename(mld = mlotst)}
# subet just the lat/depth ranges. subset months in next file
return(dx)
}
# bring in bottom depth to add to other files ##################################
# bathymetry from GLORYS
df <- #tidync::tidync(paste0(data_raw,'glorys-bathymetry.nc')) %>%
tidync::tidync(paste0('glorys-bathymetry.nc')) %>% #for Megan's set up
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude)
# bring in bottom depth to add to other files ##################################
# bathymetry from GLORYS
df <- #tidync::tidync(paste0(data_raw,'glorys-bathymetry.nc')) %>%
tidync::tidync('Data/GLORYS_Processing/glorys-bathymetry.nc') %>% #for Megan's set up
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude)
# head(df)
dt_bathy <- data.table(df)
dt_bathy = dt_bathy %>% rename(lat = latitude, lon = longitude, bottom_depth = deptho)
head(dt_bathy)
tail(dt_bathy)
# head(df)
dt_bathy <- data.table(df)
dt_bathy = dt_bathy %>% rename(lat = latitude, lon = longitude, bottom_depth = deptho)
head(dt_bathy)
tail(dt_bathy)
fwrite(dt_bathy, "Data/GLORYS_Processing/bathymetry-m.csv")
rm(df)
df1 <- tidync::tidync(paste0(data_raw, "glorys-daily-MLD-1993-01-01-2021-06-30.nc")) %>%
df1 <- tidync::tidync(paste0("glorys-daily-MLD-1993-01-01-2021-06-30.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
df1 <- tidync::tidync(paste0(data_raw, "glorys-monthly-MLD-1993-01-01-2021-06-30-1200m.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
df2 <- tidync::tidync(paste0("glorys-monthly-MLD-2022-01-01-2025-01-01-1200m.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
df2 <- tidync::tidync(paste0("Data/GLORYS_Processing/glorys-monthly-MLD-2022-01-01-2025-01-01-1200m.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt = rbindlist( list(df2)) # faster; now a data.table
# clean up for space
rm(df2)
mld_monthly <- data_prep(data.file=dt, bathy_file = dt_bathy)
rm(dt)
head(mld_monthly)
dim(mld_monthly)
mld_monthly = mld_monthly[lat>40,,]
# mld_monthly_processed for time series if included
mld_0_180 = mld_monthly[bottom_depth >= 0 & bottom_depth <=180,.(mld_0_180 = mean(mlotst)), by='date']
mld_0_90  = mld_monthly[bottom_depth >= 0 & bottom_depth <=90, .(mld_0_90 = mean(mlotst)), by='date']
mld_30_130  = mld_monthly[bottom_depth >= 30 & bottom_depth <=130 , .(mld_30_130 = mean(mlotst)), by='date']
mld_month_out = left_join(mld_0_180, mld_0_90)
mld_month_out = left_join(mld_month_out, mld_30_130)
fwrite(mld_month_out, paste0(data_dir, "glorys-monthly-means-mld2025.csv"))
rm(mld_month_out, mld_0_180, mld_0_90, mld_30_130)
fwrite(mld_month_out, "Data/GLORYS_Processing/glorys-monthly-means-mld2025.csv")
mld_month_out = left_join(mld_0_180, mld_0_90)
mld_month_out = left_join(mld_month_out, mld_30_130)
df2 <- tidync::tidync(paste0("Data/GLORYS_Processing/glorys-monthly-MLD-2022-01-01-2025-01-01-1200m.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt = rbindlist( list(df2)) # faster; now a data.table
# clean up for space
rm(df2)
mld_monthly <- data_prep(data.file=dt, bathy_file = dt_bathy)
rm(dt)
head(mld_monthly)
dim(mld_monthly)
mld_monthly = mld_monthly[lat>40,,]
# mld_monthly_processed for time series if included
mld_0_180 = mld_monthly[bottom_depth >= 0 & bottom_depth <=180,.(mld_0_180 = mean(mlotst)), by='date']
mld_0_90  = mld_monthly[bottom_depth >= 0 & bottom_depth <=90, .(mld_0_90 = mean(mlotst)), by='date']
mld_30_130  = mld_monthly[bottom_depth >= 30 & bottom_depth <=130 , .(mld_30_130 = mean(mlotst)), by='date']
mld_month_out = left_join(mld_0_180, mld_0_90)
mld_month_out = left_join(mld_month_out, mld_30_130)
fwrite(mld_month_out, "Data/GLORYS_Processing/glorys-monthly-means-mld2025.csv")
rm(mld_month_out, mld_0_180, mld_0_90, mld_30_130)
library(dplyr)
library(tidyr)
library(ggpubr)
library(corrplot)
library(mgcv)
library(DHARMa)
library(mgcViz)
library(gridExtra)
library(ROCR)
library(recdata)
library(predRecruit)
library(dplyr)
library(mgcv)
library(gratia)
library(tidyverse)
library(corrplot)
library(car)
library(gratia)
library(ggpubr)
#x = dir(data_raw)
#y = grep("monthly-crossshelf", x)
#x = x[y]
x<-"Data/GLORYS_Processing/glorys-monthly-crossshelf-eastward-1993-01-01-2021-06-30-1200m.nc"
for(i in 1:length(x)){
print(x[i])
(t1 = Sys.time())
dfx <- tidync::tidync(paste0(x[i])) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt_cst = data_prep(data.file=dfx, bathy_file = dt_bathy)
rm(dfx) # clean up memory for space
# as in temporary not temperature
# calculations for depth x lat zones = mean daily value
# subset and average by date later
cst_90_180 = dt_cst[bottom_depth >= 90 & bottom_depth <=180 , .(cst_90_180 = mean(uo)), by='date']
cst_0_180 = dt_cst[bottom_depth >= 0 & bottom_depth <=180 , .(cst_0_180 = mean(uo)), by='date']
cst_0_90 = dt_cst[bottom_depth >= 0 & bottom_depth <=90 , .(cst_0_90 = mean(uo)), by='date']
cst_30_130 = dt_cst[bottom_depth >= 30 & bottom_depth <=130 , .(cst_30_130 = mean(uo)), by='date']
cst_180_549 = dt_cst[bottom_depth >= 180 & bottom_depth <=549 , .(cst_180_549 = mean(uo)), by='date']
rm(dt_cst)
if(i==1){
CST_90_180 = cst_90_180
CST_0_180 = cst_0_180
CST_0_90 = cst_0_90
CST_30_130 = cst_30_130
CST_180_549 = cst_180_549
}else{
CST_90_180 = rbindlist( list(CST_90_180,cst_90_180))
CST_0_180 = rbindlist( list(CST_0_180,cst_0_180))
CST_0_90 = rbindlist( list(CST_0_90,cst_0_90))
CST_30_130 = rbindlist( list(CST_30_130,cst_30_130))
CST_180_549 = rbindlist( list(CST_180_549,cst_180_549))
} # end if
} # end i loop
library(tidyverse)
library(ggplot2)
library(readr)
library(data.table)
library(lubridate)
library(corrplot)
library(mgcv)
library(dplyr)
library(tidync)
#x = dir(data_raw)
#y = grep("monthly-crossshelf", x)
#x = x[y]
x<-"Data/GLORYS_Processing/glorys-monthly-crossshelf-eastward-1993-01-01-2021-06-30-1200m.nc"
for(i in 1:length(x)){
print(x[i])
(t1 = Sys.time())
dfx <- tidync::tidync(paste0(x[i])) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt_cst = data_prep(data.file=dfx, bathy_file = dt_bathy)
rm(dfx) # clean up memory for space
# as in temporary not temperature
# calculations for depth x lat zones = mean daily value
# subset and average by date later
cst_90_180 = dt_cst[bottom_depth >= 90 & bottom_depth <=180 , .(cst_90_180 = mean(uo)), by='date']
cst_0_180 = dt_cst[bottom_depth >= 0 & bottom_depth <=180 , .(cst_0_180 = mean(uo)), by='date']
cst_0_90 = dt_cst[bottom_depth >= 0 & bottom_depth <=90 , .(cst_0_90 = mean(uo)), by='date']
cst_30_130 = dt_cst[bottom_depth >= 30 & bottom_depth <=130 , .(cst_30_130 = mean(uo)), by='date']
cst_180_549 = dt_cst[bottom_depth >= 180 & bottom_depth <=549 , .(cst_180_549 = mean(uo)), by='date']
rm(dt_cst)
if(i==1){
CST_90_180 = cst_90_180
CST_0_180 = cst_0_180
CST_0_90 = cst_0_90
CST_30_130 = cst_30_130
CST_180_549 = cst_180_549
}else{
CST_90_180 = rbindlist( list(CST_90_180,cst_90_180))
CST_0_180 = rbindlist( list(CST_0_180,cst_0_180))
CST_0_90 = rbindlist( list(CST_0_90,cst_0_90))
CST_30_130 = rbindlist( list(CST_30_130,cst_30_130))
CST_180_549 = rbindlist( list(CST_180_549,cst_180_549))
} # end if
} # end i loop
library(tidyverse)
library(ggplot2)
library(readr)
library(data.table)
library(lubridate)
library(corrplot)
library(mgcv)
library(dplyr)
library(tidync)
data_prep <- function(data.file, bathy_file=NULL, shore_dist=NULL, mld=NULL, lat_min=NULL, lat_max=NULL){
dx = data.table(data.file)
# shorten some names
dx = dx %>% rename(lat = latitude, lon=longitude)
if(!is.null(lat_min)){print("subsetting by min depth") ; dx = dx[lat>=lat_min,,]}
if(!is.null(lat_max)){print("subsetting by min depth") ; dx = dx[lat<=lat_max,,]}
# time to date
print("Adding date")
dx[,date:=lubridate::as_date(dx$time/24,origin="1950-01-01 00:00:00"),]
dx[,time:=NULL] # remove time
# add bathy
if(!is.null(bathy_file)){
print("Adding bottom depth")
# bathy_file[,date:=lubridate::as_date(bathy_file$date/24,origin="1950-01-01 00:00:00"),]
dx = merge.data.table(dx, bathy_file, by=c("lat","lon"))}
if(!is.null(shore_dist)){
print("Adding distance to shore")
# shore_dist[,date:=lubridate::as_date(shore_dist$date/24,origin="1950-01-01 00:00:00"),]
dx = merge.data.table(dx, shore_dist, by=c('lat','lon'))}
if(!is.null(mld)){
print("Adding MLD")
# mld[,date:=lubridate::as_date(mld$date/24,origin="1950-01-01 00:00:00"),]
mld[,bottom_depth:=NULL,] # remove bottome depth from mld file.
# if not included in the join. messes up join for some reason
dx = merge.data.table(dx, mld, by=c('lat','lon', 'date'))
dx = dx %>% rename(mld = mlotst)}
# subet just the lat/depth ranges. subset months in next file
return(dx)
}
# bring in bottom depth to add to other files ##################################
# bathymetry from GLORYS
df <- #tidync::tidync(paste0(data_raw,'glorys-bathymetry.nc')) %>%
tidync::tidync('Data/GLORYS_Processing/glorys-bathymetry.nc') %>% #for Megan's set up
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude)
# head(df)
dt_bathy <- data.table(df)
dt_bathy = dt_bathy %>% rename(lat = latitude, lon = longitude, bottom_depth = deptho)
head(dt_bathy)
tail(dt_bathy)
fwrite(dt_bathy, "Data/GLORYS_Processing/bathymetry-m.csv")
rm(df)
df1 <- tidync::tidync("Data/GLORYS_Processing/glorys-monthly-MLD-1993-01-01-2021-06-30-1200m.nc") %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
df2 <- tidync::tidync(paste0("Data/GLORYS_Processing/glorys-monthly-MLD-2022-01-01-2025-01-01-1200m.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt = rbindlist( list(df2)) # faster; now a data.table
# clean up for space
rm(df2)
mld_monthly <- data_prep(data.file=dt, bathy_file = dt_bathy)
dt$time
dt = rbindlist( list(df1,df2)) # faster; now a data.table
df1 <- tidync::tidync("Data/GLORYS_Processing/glorys-monthly-MLD-1993-01-01-2021-06-30-1200m.nc") %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
df2 <- tidync::tidync(paste0("Data/GLORYS_Processing/glorys-monthly-MLD-2022-01-01-2025-01-01-1200m.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt = rbindlist( list(df1,df2)) # faster; now a data.table
# clean up for space
#rm(df2)
dt$time
library(tidyverse)
library(ggplot2)
library(readr)
library(data.table)
library(lubridate)
library(corrplot)
library(mgcv)
library(dplyr)
library(tidync)
data_prep <- function(data.file, bathy_file=NULL, shore_dist=NULL, mld=NULL, lat_min=NULL, lat_max=NULL){
dx = data.table(data.file)
# shorten some names
dx = dx %>% rename(lat = latitude, lon=longitude)
if(!is.null(lat_min)){print("subsetting by min depth") ; dx = dx[lat>=lat_min,,]}
if(!is.null(lat_max)){print("subsetting by min depth") ; dx = dx[lat<=lat_max,,]}
# time to date
print("Adding date")
dx[,date:=lubridate::as_date(dx$time/24,origin="1950-01-01 00:00:00"),]
dx[,time:=NULL] # remove time
# add bathy
if(!is.null(bathy_file)){
print("Adding bottom depth")
# bathy_file[,date:=lubridate::as_date(bathy_file$date/24,origin="1950-01-01 00:00:00"),]
dx = merge.data.table(dx, bathy_file, by=c("lat","lon"))}
if(!is.null(shore_dist)){
print("Adding distance to shore")
# shore_dist[,date:=lubridate::as_date(shore_dist$date/24,origin="1950-01-01 00:00:00"),]
dx = merge.data.table(dx, shore_dist, by=c('lat','lon'))}
if(!is.null(mld)){
print("Adding MLD")
# mld[,date:=lubridate::as_date(mld$date/24,origin="1950-01-01 00:00:00"),]
mld[,bottom_depth:=NULL,] # remove bottome depth from mld file.
# if not included in the join. messes up join for some reason
dx = merge.data.table(dx, mld, by=c('lat','lon', 'date'))
dx = dx %>% rename(mld = mlotst)}
# subet just the lat/depth ranges. subset months in next file
return(dx)
}
# bring in bottom depth to add to other files ##################################
# bathymetry from GLORYS
df <- #tidync::tidync(paste0(data_raw,'glorys-bathymetry.nc')) %>%
tidync::tidync('Data/GLORYS_Processing/glorys-bathymetry.nc') %>% #for Megan's set up
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude)
# head(df)
dt_bathy <- data.table(df)
dt_bathy = dt_bathy %>% rename(lat = latitude, lon = longitude, bottom_depth = deptho)
head(dt_bathy)
tail(dt_bathy)
fwrite(dt_bathy, "Data/GLORYS_Processing/bathymetry-m.csv")
rm(df)
df1 <- tidync::tidync("Data/GLORYS_Processing/glorys-monthly-MLD-1993-01-01-2021-06-30-1200m.nc") %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
df2 <- tidync::tidync(paste0("Data/GLORYS_Processing/glorys-monthly-MLD-2022-01-01-2025-01-01-1200m.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt = rbindlist( list(df1,df2)) # faster; now a data.table
# clean up for space
#rm(df2)
dt$time
mld_monthly <- data_prep(data.file=dt, bathy_file = dt_bathy)
write.csv(dt,"MLD_combined.csv")
# get names of data files
x = dir(data_raw)
y = grep("monthly-longshore" , x)
x<-"Data/GLORYS_Processing/glorys-monthly-crossshelf-eastward-1993-01-01-2021-06-30-1200m.nc"
x
x<-"Data/GLORYS_Processing/glorys-monthly-crossshelf-eastward-2021-07-01-2025-03-01-1200m.nc"
for(i in 1:length(x)){
print(x[i])
(t1 = Sys.time())
dfx <- tidync::tidync(paste0(x[i])) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt_cst = data_prep(data.file=dfx, bathy_file = dt_bathy)
rm(dfx) # clean up memory for space
# as in temporary not temperature
# calculations for depth x lat zones = mean daily value
# subset and average by date later
cst_90_180 = dt_cst[bottom_depth >= 90 & bottom_depth <=180 , .(cst_90_180 = mean(uo)), by='date']
cst_0_180 = dt_cst[bottom_depth >= 0 & bottom_depth <=180 , .(cst_0_180 = mean(uo)), by='date']
cst_0_90 = dt_cst[bottom_depth >= 0 & bottom_depth <=90 , .(cst_0_90 = mean(uo)), by='date']
cst_30_130 = dt_cst[bottom_depth >= 30 & bottom_depth <=130 , .(cst_30_130 = mean(uo)), by='date']
cst_180_549 = dt_cst[bottom_depth >= 180 & bottom_depth <=549 , .(cst_180_549 = mean(uo)), by='date']
rm(dt_cst)
if(i==1){
CST_90_180 = cst_90_180
CST_0_180 = cst_0_180
CST_0_90 = cst_0_90
CST_30_130 = cst_30_130
CST_180_549 = cst_180_549
}else{
CST_90_180 = rbindlist( list(CST_90_180,cst_90_180))
CST_0_180 = rbindlist( list(CST_0_180,cst_0_180))
CST_0_90 = rbindlist( list(CST_0_90,cst_0_90))
CST_30_130 = rbindlist( list(CST_30_130,cst_30_130))
CST_180_549 = rbindlist( list(CST_180_549,cst_180_549))
} # end if
} # end i loop
cst_monthly = left_join(CST_90_180,CST_0_180)
cst_monthly = left_join(cst_monthly,CST_0_90)
cst_monthly = left_join(cst_monthly,CST_30_130)
cst_monthly = left_join(cst_monthly,CST_180_549)
# cst_monthly_late <- cst_monthly
# clean up to save memory
fwrite(cst_monthly, "Data/GLORYS_Processing/glorys-monthly-means-cst-2025.csv")
