ROMS$H17a = aggregate(H17a ~ Year, data=roms[roms$Month %in% 2:5,], FUN=sum)[,2]            #H21a
ROMS$version = ifelse(ROMS$Year<2011,'old','new')
# head(ROMS)
colnames(ROMS) = c('Year','DDpre','CSTegg','DDegg','LSTyolk','DDlarv','Version')
##### BRING IN FISH DATA AND PROCESS A BIT ####
#### data through 2015 from the 2016 assessment ####
fish2016 = data.frame(read.table('SablefishData2015.csv', header=TRUE, sep=','))
h = 0.6
Ro = 17198
So = 115622
b = (Ro*0.2*So)/(h*Ro)-(0.2*So)
S = fish2016$sp.bio
R.pred <- (Ro*S/(b+S))
R.ass = fish2016$age.0 # recruitment from the 20216 assessment
resids = R.ass - R.pred
# resids
fish2016$resids = resids
# head(fish2016)
colnames(fish2016)[1] = 'Year'
#### bring in data from the 2019 stock assessement ####
fish2019 <- data.frame((read.csv("Src/NicksSablefishComparison/Data_All_Proccessed_Data.csv", header=TRUE )))
fish2019 <- data.frame((read.csv("Data_All_Proccessed_Data.csv", header=TRUE )))
fishyy = fish2019[fish2019$year %in% 1981:2019,c('year','resids','Rec_Dev_Value')]
colnames(fishyy) <- c('Year','resids2019','recdev2019')
fishy = merge(ROMS, fishyy, by = 'Year')
fish <- merge(fishy, fish2016, by = "Year", all=TRUE)
# head(fish)
# tail(fish)
write.csv(fish,'D_Fish_ROMS_north_update.csv', row.names = FALSE)
par( mfrow = c(3,2), mar = c(4,4,1,1))
fish$years = ifelse(fish$Version == 'old',"1980-2010","2011-2018")
plot( as.factor(fish$years), fish$DDpre, main = "DDpre" )
plot( as.factor(fish$years), fish$DDegg, main = "DDegg" )
plot( as.factor(fish$years), fish$CSTegg, main = "CSTeg" )
plot( as.factor(fish$years), fish$LSTyolk, main = "LSTyolk" )
plot( as.factor(fish$years), fish$DDlarv, main = "DDlarv" )
par(mfrow = c(3,2), mar = c(4,4,1,1))
yrs = 1980:2020
plot(fish$Year,fish$DDpre,type='l', xlab = NA, xlim = c(min(yrs), max(yrs)), ylab = 'DDpre')
axis(side=1, at = yrs, tck=-0.02, labels=NA)
plot(fish$Year,fish$DDegg,type='l', xlab = NA, xlim = c(min(yrs), max(yrs)), ylab = 'DDegg')
axis(side=1, at = yrs, tck=-0.02, labels=NA)
plot(fish$Year,fish$CSTegg,type='l', xlab = NA, xlim = c(min(yrs), max(yrs)), ylab = 'CSTegg')
axis(side=1, at = yrs, tck=-0.02, labels=NA)
plot(fish$Year,fish$LSTyolk,type='l', xlab = NA, xlim = c(min(yrs), max(yrs)), ylab = 'LSTyolk')
axis(side=1, at = yrs, tck=-0.02, labels=NA)
plot(fish$Year,fish$DDlarv,type='l', xlab = NA, xlim = c(min(yrs), max(yrs)), ylab = 'DDlarv')
axis(side=1, at = yrs, tck=-0.02, labels=NA)
#### 2016 vs 2019 data ####
x = fish$resids[fish$Year %in% 1981:2010]
y = fish$resids2019[fish$Year %in% 1981:2010]
r_2016_vs_2019 = round(cor(x,y),2)
library(vegan)
install.packages("vegan")
par(mfrow = c(3,2), mar = c(4,4,1,1))
yrs = 1980:2020
plot(fish$Year,fish$DDpre,type='l', xlab = NA, xlim = c(min(yrs), max(yrs)), ylab = 'DDpre')
axis(side=1, at = yrs, tck=-0.02, labels=NA)
plot(fish$Year,fish$DDegg,type='l', xlab = NA, xlim = c(min(yrs), max(yrs)), ylab = 'DDegg')
axis(side=1, at = yrs, tck=-0.02, labels=NA)
plot(fish$Year,fish$CSTegg,type='l', xlab = NA, xlim = c(min(yrs), max(yrs)), ylab = 'CSTegg')
axis(side=1, at = yrs, tck=-0.02, labels=NA)
plot(fish$Year,fish$LSTyolk,type='l', xlab = NA, xlim = c(min(yrs), max(yrs)), ylab = 'LSTyolk')
axis(side=1, at = yrs, tck=-0.02, labels=NA)
plot(fish$Year,fish$DDlarv,type='l', xlab = NA, xlim = c(min(yrs), max(yrs)), ylab = 'DDlarv')
axis(side=1, at = yrs, tck=-0.02, labels=NA)
#### 2016 vs 2019 data ####
x = fish$resids[fish$Year %in% 1981:2010]
y = fish$resids2019[fish$Year %in% 1981:2010]
r_2016_vs_2019 = round(cor(x,y),2)
library(vegan)
dx = fish[fish$Year %in% 1981:2018, c('DDpre','CSTegg','DDegg','LSTyolk','DDlarv')]
# normalize data
dy = apply(dx, 2, scale)
dc = ifelse(fish$Year[fish$Year %in% 1981:2018] < 2011, "white", "black")
# untransformed values
m1 = metaMDS(dx, distance = 'euclidean')
par(mfrow = c(2,2), par = c(2,1,1,1))
ordiplot(m1 , xlim = c(-75,75), main = "Untransformed ROMS")
x = as.numeric(m1$points[,1])
y = as.numeric(m1$points[,2])
points(x,y, pch=21, bg = dc)
ordiplot(m1 , type='none', xlim = c(-75,75))
text(x,y, 1981:2018, cex = 0.7)
# normalized time series
m1 = metaMDS(dy, distance = 'euclidean')
ordiplot(m1,xlim = c(-5,5), main = "Normalized ROMS")
x = as.numeric(m1$points[,1])
y = as.numeric(m1$points[,2])
points(x,y, pch=21, bg = dc)
ordiplot(m1, xlim = c(-5,5), type='none')
text(x,y, 1981:2018, cex = 0.7)
Full_Glory_Yellow_Tail.df <- read.csv(
here("Data", "Yellowtail", "2024Env-annual-yellowtail_GLORYS_UNSTANDARDIZED.csv")
)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(here)
library(patchwork)
Full_Glory_Yellow_Tail.df <- read.csv(
here("Data", "Yellowtail", "2024Env-annual-yellowtail_GLORYS_UNSTANDARDIZED.csv")
)
head(Full_Glory_Yellow_Tail.df)
Full_Glory_Yellow_Tail.df <- read.csv(
here("Data", "Yellowtail", "2024Env-annual-yellowtail_GLORYS_UNSTANDARDIZED.csv")
)
head(Full_Glory_Yellow_Tail.df)
scale(Full_Glory_Yellow_Tail.df)
scaled <- scale(Full_Glory_Yellow_Tail.df)
colMeans(scaled)
colMeans(na.omit(scaled))
scale(Full_Glory_Yellow_Tail.df)
scaled <- scale(Full_Glory_Yellow_Tail.df)
mean(scaled$Year)
scaled <- data.frame(scale(Full_Glory_Yellow_Tail.df))
mean(scaled$Year)
colMeans(na.omit(scaled))
scaled <- data.frame(scale(Full_Glory_Yellow_Tail.df))
mean(scaled$Year)
mean(na.omit(scaled$Year))
mean(na.omit(scaled$bakun_sti))
ncol(Full_Glory_Yellow_Tail.df)
Full_Glory_Yellow_Tail.df
Full_Glory_Yellow_Tail.df <- read.csv(
here("Data", "Yellowtail", "2024Env-annual-yellowtail_GLORYS_UNSTANDARDIZED.csv")
)%>%
filter(year>=1994&year<=2024)
Full_Glory_Yellow_Tail.df
dat<- read.csv(
here("Data", "Yellowtail", "2024Env-annual-yellowtail_GLORYS_UNSTANDARDIZED.csv")
)%>%
filter(year>=1994&year<=2024) # lets filter the data so we dont have any missing values
dat[,i]
dat[,1]
i<-1
mean(dat[,i])
(dat[,i]-mean(dat[,i]))/sd(dat[,i])
standardized[,i]<-(dat[,i]-mean(dat[,i]))/sd(dat[,i])
(dat[,i]-mean(dat[,i]))/sd(dat[,i])
standardized <- data.frame() # a
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(Full_Glory_Yellow_Tail.df){
mean[,i]
dat[,i]-mean(dat[,i])
(dat[,i]-mean(dat[,i]))/sd(dat[,i]
}
(dat[,i]-mean(dat[,i]))/sd(dat[,i]
(dat[,i]-mean(dat[,i]))/sd(dat[,i]
(dat[,i]-mean(dat[,i]))/sd(dat[,i])
(dat[,i]-mean(dat[,i]))/sd(dat[,i])
standardized[,i]<-(dat[,i]-mean(dat[,i]))/sd(dat[,i])
standardized[,i]<-(dat[,i]-mean(dat[,i]))/sd(dat[,i])
standardized <- NA # a
standardized[,i]<-(dat[,i]-mean(dat[,i]))/sd(dat[,i])
standardized[,i]
standardized <-data.frame(ncol=ncol(Full_Glory_Yellow_Tail.df))# a
standardized[,i]<-(dat[,i]-mean(dat[,i]))/sd(dat[,i])
stand_dat <-data.frame()
(dat[,i]-mean(dat[,i]))/sd(dat[,i])
(dat[,i]-mean(dat[,i]))/sd(dat[,i])
stand_dat[1:31,i]<-(dat[,i]-mean(dat[,i]))/sd(dat[,i])
for(i in 1:ncol(dat){
for(i in 1:ncol(dat){
stand_dat[1:31,i]<- dat[,i]-mean(dat[,i]/sd(dat[,i])
}
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i])/sd(dat[,i])
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i])/sd(dat[,i])
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i])/sd(dat[,i])
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i])/sd(dat[,i])
stand_dat <-data.frame()
for(i in 1:ncol(dat){
for(i in 1:ncol(dat)){
stand_dat
stand_dat <-data.frame()
#i is what we want the loop to iterate over, in this case i is columns. If we want the
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
for(i in 1:ncol(dat)){
}
stand_dat
(dat[,i]-mean(dat[,i])/sd(dat[,i])
(dat[,i]-mean(dat[,i])/sd(dat[,i])
(dat[,i]-mean(dat[,i])/sd(dat[,i])
mean(dat[,i])
mean(dat[,i])
dat[,i]-mean(dat[,i])
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i]
}
for(i in 1:ncol(dat)){
stand_dat <-data.frame()
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i]
}
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i]
}
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i]
(dat[,i]-mean(dat[,i]))/sd(dat[,i])
stand_dat[1:31,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
stand_dat <-data.frame()
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat <-ls()
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat <-list()
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat <-NA
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat <-NA
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
stand_dat<- cbind(stand_dat,stand_dat)
}
stand_dat
stand_dat <-NA
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
temp<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
stand_dat<- cbind(stand_dat,temp)
}
stand_dat
stand_dat <-dat
stand_dat <-dat
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat
stand_dat <-dat
#i is what we want the loop to iterate over, in this case i is columns. If we want the
# loop to loop over every column, we can start it at 1 and go to the total number
# of columns. ncol() is a function that counts the number of columns in a dataframe
# so in this case we will loop from column 1 to the last column
for(i in 1:ncol(dat)){
stand_dat[,i]<- (dat[,i]-mean(dat[,i]))/sd(dat[,i])
}
stand_dat
colMeans(stand_dat)
apply(stand_dat, 2, sd)
colMeans(stand_dat)
apply(dat, 2, scale)
stand_data_apply <- apply(dat, 2, scale)
stand_data_apply - stand_dat
diff<-stand_data_apply - stand_dat
head(diff)
head(stand_data_apply )
scale_dat_scale <- scale(dat)
apply(scale_dat_scale, 2, sd)
apply(na.omit(scale_dat_scale), 2, sd)
apply(scale_dat_scale, 2, sd)
View(dat)
library(tidyverse)
library(ggplot2)
library(readr)
library(data.table)
library(lubridate)
library(corrplot)
library(mgcv)
library(dplyr)
library(tidync)
data_prep <- function(data.file, bathy_file=NULL, shore_dist=NULL, mld=NULL, lat_min=NULL, lat_max=NULL){
dx = data.table(data.file)
# shorten some names
dx = dx %>% rename(lat = latitude, lon=longitude)
if(!is.null(lat_min)){print("subsetting by min depth") ; dx = dx[lat>=lat_min,,]}
if(!is.null(lat_max)){print("subsetting by min depth") ; dx = dx[lat<=lat_max,,]}
# time to date
print("Adding date")
dx[,date:=lubridate::as_date(dx$time/24,origin="1950-01-01 00:00:00"),]
dx[,time:=NULL] # remove time
# add bathy
if(!is.null(bathy_file)){
print("Adding bottom depth")
# bathy_file[,date:=lubridate::as_date(bathy_file$date/24,origin="1950-01-01 00:00:00"),]
dx = merge.data.table(dx, bathy_file, by=c("lat","lon"))}
if(!is.null(shore_dist)){
print("Adding distance to shore")
# shore_dist[,date:=lubridate::as_date(shore_dist$date/24,origin="1950-01-01 00:00:00"),]
dx = merge.data.table(dx, shore_dist, by=c('lat','lon'))}
if(!is.null(mld)){
print("Adding MLD")
# mld[,date:=lubridate::as_date(mld$date/24,origin="1950-01-01 00:00:00"),]
mld[,bottom_depth:=NULL,] # remove bottome depth from mld file.
# if not included in the join. messes up join for some reason
dx = merge.data.table(dx, mld, by=c('lat','lon', 'date'))
dx = dx %>% rename(mld = mlotst)}
# subet just the lat/depth ranges. subset months in next file
return(dx)
}
# bring in bottom depth to add to other files ##################################
# bathymetry from GLORYS
df <- #tidync::tidync(paste0(data_raw,'glorys-bathymetry.nc')) %>%
tidync::tidync(paste0('glorys-bathymetry.nc')) %>% #for Megan's set up
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude)
# bring in bottom depth to add to other files ##################################
# bathymetry from GLORYS
df <- #tidync::tidync(paste0(data_raw,'glorys-bathymetry.nc')) %>%
tidync::tidync('Data/GLORYS_Processing/glorys-bathymetry.nc') %>% #for Megan's set up
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude)
# head(df)
dt_bathy <- data.table(df)
dt_bathy = dt_bathy %>% rename(lat = latitude, lon = longitude, bottom_depth = deptho)
head(dt_bathy)
tail(dt_bathy)
# head(df)
dt_bathy <- data.table(df)
dt_bathy = dt_bathy %>% rename(lat = latitude, lon = longitude, bottom_depth = deptho)
head(dt_bathy)
tail(dt_bathy)
fwrite(dt_bathy, "Data/GLORYS_Processing/bathymetry-m.csv")
rm(df)
df1 <- tidync::tidync(paste0(data_raw, "glorys-daily-MLD-1993-01-01-2021-06-30.nc")) %>%
df1 <- tidync::tidync(paste0("glorys-daily-MLD-1993-01-01-2021-06-30.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
df1 <- tidync::tidync(paste0(data_raw, "glorys-monthly-MLD-1993-01-01-2021-06-30-1200m.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
df2 <- tidync::tidync(paste0("glorys-monthly-MLD-2022-01-01-2025-01-01-1200m.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
df2 <- tidync::tidync(paste0("Data/GLORYS_Processing/glorys-monthly-MLD-2022-01-01-2025-01-01-1200m.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt = rbindlist( list(df2)) # faster; now a data.table
# clean up for space
rm(df2)
mld_monthly <- data_prep(data.file=dt, bathy_file = dt_bathy)
rm(dt)
head(mld_monthly)
dim(mld_monthly)
mld_monthly = mld_monthly[lat>40,,]
# mld_monthly_processed for time series if included
mld_0_180 = mld_monthly[bottom_depth >= 0 & bottom_depth <=180,.(mld_0_180 = mean(mlotst)), by='date']
mld_0_90  = mld_monthly[bottom_depth >= 0 & bottom_depth <=90, .(mld_0_90 = mean(mlotst)), by='date']
mld_30_130  = mld_monthly[bottom_depth >= 30 & bottom_depth <=130 , .(mld_30_130 = mean(mlotst)), by='date']
mld_month_out = left_join(mld_0_180, mld_0_90)
mld_month_out = left_join(mld_month_out, mld_30_130)
fwrite(mld_month_out, paste0(data_dir, "glorys-monthly-means-mld2025.csv"))
rm(mld_month_out, mld_0_180, mld_0_90, mld_30_130)
fwrite(mld_month_out, "Data/GLORYS_Processing/glorys-monthly-means-mld2025.csv")
mld_month_out = left_join(mld_0_180, mld_0_90)
mld_month_out = left_join(mld_month_out, mld_30_130)
df2 <- tidync::tidync(paste0("Data/GLORYS_Processing/glorys-monthly-MLD-2022-01-01-2025-01-01-1200m.nc")) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt = rbindlist( list(df2)) # faster; now a data.table
# clean up for space
rm(df2)
mld_monthly <- data_prep(data.file=dt, bathy_file = dt_bathy)
rm(dt)
head(mld_monthly)
dim(mld_monthly)
mld_monthly = mld_monthly[lat>40,,]
# mld_monthly_processed for time series if included
mld_0_180 = mld_monthly[bottom_depth >= 0 & bottom_depth <=180,.(mld_0_180 = mean(mlotst)), by='date']
mld_0_90  = mld_monthly[bottom_depth >= 0 & bottom_depth <=90, .(mld_0_90 = mean(mlotst)), by='date']
mld_30_130  = mld_monthly[bottom_depth >= 30 & bottom_depth <=130 , .(mld_30_130 = mean(mlotst)), by='date']
mld_month_out = left_join(mld_0_180, mld_0_90)
mld_month_out = left_join(mld_month_out, mld_30_130)
fwrite(mld_month_out, "Data/GLORYS_Processing/glorys-monthly-means-mld2025.csv")
rm(mld_month_out, mld_0_180, mld_0_90, mld_30_130)
library(dplyr)
library(tidyr)
library(ggpubr)
library(corrplot)
library(mgcv)
library(DHARMa)
library(mgcViz)
library(gridExtra)
library(ROCR)
library(recdata)
library(predRecruit)
library(dplyr)
library(mgcv)
library(gratia)
library(tidyverse)
library(corrplot)
library(car)
library(gratia)
library(ggpubr)
#x = dir(data_raw)
#y = grep("monthly-crossshelf", x)
#x = x[y]
x<-"Data/GLORYS_Processing/glorys-monthly-crossshelf-eastward-1993-01-01-2021-06-30-1200m.nc"
for(i in 1:length(x)){
print(x[i])
(t1 = Sys.time())
dfx <- tidync::tidync(paste0(x[i])) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt_cst = data_prep(data.file=dfx, bathy_file = dt_bathy)
rm(dfx) # clean up memory for space
# as in temporary not temperature
# calculations for depth x lat zones = mean daily value
# subset and average by date later
cst_90_180 = dt_cst[bottom_depth >= 90 & bottom_depth <=180 , .(cst_90_180 = mean(uo)), by='date']
cst_0_180 = dt_cst[bottom_depth >= 0 & bottom_depth <=180 , .(cst_0_180 = mean(uo)), by='date']
cst_0_90 = dt_cst[bottom_depth >= 0 & bottom_depth <=90 , .(cst_0_90 = mean(uo)), by='date']
cst_30_130 = dt_cst[bottom_depth >= 30 & bottom_depth <=130 , .(cst_30_130 = mean(uo)), by='date']
cst_180_549 = dt_cst[bottom_depth >= 180 & bottom_depth <=549 , .(cst_180_549 = mean(uo)), by='date']
rm(dt_cst)
if(i==1){
CST_90_180 = cst_90_180
CST_0_180 = cst_0_180
CST_0_90 = cst_0_90
CST_30_130 = cst_30_130
CST_180_549 = cst_180_549
}else{
CST_90_180 = rbindlist( list(CST_90_180,cst_90_180))
CST_0_180 = rbindlist( list(CST_0_180,cst_0_180))
CST_0_90 = rbindlist( list(CST_0_90,cst_0_90))
CST_30_130 = rbindlist( list(CST_30_130,cst_30_130))
CST_180_549 = rbindlist( list(CST_180_549,cst_180_549))
} # end if
} # end i loop
library(tidyverse)
library(ggplot2)
library(readr)
library(data.table)
library(lubridate)
library(corrplot)
library(mgcv)
library(dplyr)
library(tidync)
#x = dir(data_raw)
#y = grep("monthly-crossshelf", x)
#x = x[y]
x<-"Data/GLORYS_Processing/glorys-monthly-crossshelf-eastward-1993-01-01-2021-06-30-1200m.nc"
for(i in 1:length(x)){
print(x[i])
(t1 = Sys.time())
dfx <- tidync::tidync(paste0(x[i])) %>%
hyper_tibble( force = TRUE) %>%
drop_na() %>%
group_by(longitude,latitude,time)
dt_cst = data_prep(data.file=dfx, bathy_file = dt_bathy)
rm(dfx) # clean up memory for space
# as in temporary not temperature
# calculations for depth x lat zones = mean daily value
# subset and average by date later
cst_90_180 = dt_cst[bottom_depth >= 90 & bottom_depth <=180 , .(cst_90_180 = mean(uo)), by='date']
cst_0_180 = dt_cst[bottom_depth >= 0 & bottom_depth <=180 , .(cst_0_180 = mean(uo)), by='date']
cst_0_90 = dt_cst[bottom_depth >= 0 & bottom_depth <=90 , .(cst_0_90 = mean(uo)), by='date']
cst_30_130 = dt_cst[bottom_depth >= 30 & bottom_depth <=130 , .(cst_30_130 = mean(uo)), by='date']
cst_180_549 = dt_cst[bottom_depth >= 180 & bottom_depth <=549 , .(cst_180_549 = mean(uo)), by='date']
rm(dt_cst)
if(i==1){
CST_90_180 = cst_90_180
CST_0_180 = cst_0_180
CST_0_90 = cst_0_90
CST_30_130 = cst_30_130
CST_180_549 = cst_180_549
}else{
CST_90_180 = rbindlist( list(CST_90_180,cst_90_180))
CST_0_180 = rbindlist( list(CST_0_180,cst_0_180))
CST_0_90 = rbindlist( list(CST_0_90,cst_0_90))
CST_30_130 = rbindlist( list(CST_30_130,cst_30_130))
CST_180_549 = rbindlist( list(CST_180_549,cst_180_549))
} # end if
} # end i loop
